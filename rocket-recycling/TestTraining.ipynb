{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rocket import Rocket\n",
    "from TestNetwork import ActorCritic\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_episode(max_steps, environment, network):\n",
    "\n",
    "    # rewards, log_probs, values, masks = [], [], [], []\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    terminated = False\n",
    "\n",
    "    state = env.reset() \n",
    "    terminated = False\n",
    "    for step_id in range(max_steps):\n",
    "        \n",
    "        action, log_prob, value = net.get_action(state)\n",
    "        state, reward, terminated, _ = environment.step(action) \n",
    "    \n",
    "        # rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        masks.append(1-terminated)\n",
    "        states.append(torch.FloatTensor(state))\n",
    "        rewards.append(reward)\n",
    "        actions.append(torch.FloatTensor(action))\n",
    "\n",
    "        \n",
    "\n",
    "        if terminated or step_id == max_steps-1:\n",
    "            _, _, Qval = network.get_action(state)\n",
    "            network.update_ac(network, actions, states, rewards, log_probs, values, masks, Qval, gamma=0.999)\n",
    "            break\n",
    "    return states, actions, rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [8] at entry 0 and [0] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#actor_critic implementation:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m net\u001b[38;5;241m.\u001b[39mactor_target\u001b[38;5;241m.\u001b[39mload_state_dict(net\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[0;32m---> 33\u001b[0m states, actions, rewards \u001b[38;5;241m=\u001b[39m \u001b[43mgen_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m REWARDS\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39msum(rewards))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode id: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, episode reward: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;241m%\u001b[39m (episode_id, np\u001b[38;5;241m.\u001b[39msum(rewards)))\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mgen_episode\u001b[0;34m(max_steps, environment, network)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m step_id \u001b[38;5;241m==\u001b[39m max_steps\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     27\u001b[0m         _, _, Qval \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[0;32m---> 28\u001b[0m         \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_ac\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m states, actions, rewards\n",
      "File \u001b[0;32m~/Desktop/KAIST_Y4S2/강화학습개론/RL_FinalProject/rocket-recycling/TestNetwork.py:128\u001b[0m, in \u001b[0;36mActorCritic.update_ac\u001b[0;34m(network, actions, states, rewards, log_probs, values, masks, Qval, gamma)\u001b[0m\n\u001b[1;32m    124\u001b[0m values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(values)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# print('log probs:', log_probs)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# print('values:', values)\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m action_tuple \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m state_tuple \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(states)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_tuple\u001b[39m\u001b[38;5;124m'\u001b[39m, action_tuple)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [8] at entry 0 and [0] at entry 1"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    task = 'hover'  # 'hover' or 'landing'\n",
    "    version = 1\n",
    "\n",
    "    max_m_episode = 200000\n",
    "    max_steps = 800\n",
    "\n",
    "    \n",
    "    env = Rocket(task=task, max_steps=max_steps)\n",
    "    ckpt_folder = os.path.join('./', task + '_ckpt')\n",
    "    if not os.path.exists(ckpt_folder):\n",
    "        os.mkdir(ckpt_folder)\n",
    "\n",
    "    last_episode_id = 0\n",
    "    REWARDS = []\n",
    "    net = ActorCritic(input_dim=env.state_dims, output_dim=env.action_dims).to(device)\n",
    "    if len(glob.glob(os.path.join(ckpt_folder, '*.pt'))) > 0:\n",
    "            # load the last ckpt\n",
    "        checkpoint = torch.load(glob.glob(os.path.join(ckpt_folder, '*.pt'))[-1])\n",
    "        net.load_state_dict(checkpoint['model_G_state_dict'])\n",
    "        last_episode_id = checkpoint['episode_id']\n",
    "        REWARDS = checkpoint['REWARDS']\n",
    "\n",
    "\n",
    "    for episode_id in range(last_episode_id, max_m_episode):\n",
    "\n",
    "            # training loop\n",
    "        state = env.reset()\n",
    "        rewards, log_probs, values, masks = [], [], [], []\n",
    "            #actor_critic implementation:\n",
    "        net.actor_target.load_state_dict(net.actor.state_dict())\n",
    "        states, actions, rewards = gen_episode(max_steps=max_steps, environment = env, network = net)\n",
    "\n",
    "        REWARDS.append(np.sum(rewards))\n",
    "        print('episode id: %d, episode reward: %.3f'\n",
    "                % (episode_id, np.sum(rewards)))\n",
    "\n",
    "        if episode_id % 100 == 1:\n",
    "            plt.figure()\n",
    "            plt.plot(REWARDS), plt.plot(utils.moving_avg(REWARDS, N=50))\n",
    "            plt.legend(['episode reward', 'moving avg'], loc=2)\n",
    "            plt.xlabel('m episode')\n",
    "            plt.ylabel('reward')\n",
    "            plt.savefig(os.path.join(ckpt_folder, 'rewards_' + str(version).zfill(8) + '.jpg'))\n",
    "            plt.close()\n",
    "\n",
    "            torch.save({'episode_id': episode_id,\n",
    "                            'REWARDS': REWARDS,\n",
    "                            'model_G_state_dict': net.state_dict()},\n",
    "                           os.path.join(ckpt_folder, 'ckpt_' + str(version).zfill(8) + '.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
