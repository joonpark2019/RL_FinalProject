{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rocket import Rocket\n",
    "# from TestNetwork import ActorCritic\n",
    "# from PPO_network import VNetwork, PolicyNetwork\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "from collections import deque\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalMapping(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional mapping Layer.\n",
    "    This layer map continuous input coordinates into a higher dimensional space\n",
    "    and enable the prediction to more easily approximate a higher frequency function.\n",
    "    See NERF paper for more details (https://arxiv.org/pdf/2003.08934.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, L=5, scale=1.0):\n",
    "        super(PositionalMapping, self).__init__()\n",
    "        self.L = L\n",
    "        self.output_dim = input_dim * (L*2 + 1)\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x * self.scale\n",
    "\n",
    "        if self.L == 0:\n",
    "            return x\n",
    "\n",
    "        h = [x]\n",
    "        PI = 3.1415927410125732\n",
    "        for i in range(self.L):\n",
    "            x_sin = torch.sin(2**i * PI * x)\n",
    "            x_cos = torch.cos(2**i * PI * x)\n",
    "            h.append(x_sin)\n",
    "            h.append(x_cos)\n",
    "\n",
    "        return torch.cat(h, dim=-1) / self.scale\n",
    "\n",
    "# Based on the code from https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "# Network architecture and hyperparameters are based on : https://arxiv.org/pdf/2006.05990.pdf\n",
    "# The code below is taken from: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class NetworkLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.mapping = PositionalMapping(input_dim=input_dim, L=7)\n",
    "        self.layer = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.mapping.output_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, output_dim), std=1.0),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view([1, -1])\n",
    "        x =  self.mapping(x)\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = NetworkLayer(state_dim, action_dim)\n",
    "        self.actor_target = NetworkLayer(state_dim, action_dim)\n",
    "        self.critic = NetworkLayer(state_dim, 1)\n",
    "\n",
    "    def gen_action(self, x):\n",
    "    \n",
    "    def update(self, states, actions, rewards):\n",
    "\n",
    "\n",
    "class VNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.mapping = PositionalMapping(input_dim=input_dim, L=7)\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.mapping.output_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view([1, -1])\n",
    "        x = self.mapping(x)\n",
    "        x = self.critic(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.mapping = PositionalMapping(input_dim=input_dim, L=7)\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.mapping.output_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, output_dim), std=1.0),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view([1, -1])\n",
    "        x = self.mapping(x)\n",
    "        x = self.actor(x)\n",
    "        x = torch.nn.functional.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def gen_episode(environment, policy_target, device, max_step = 800):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    ratios = []\n",
    "    state = environment.reset() \n",
    "    terminated = False\n",
    "\n",
    "    for step in range(max_step):\n",
    "        probs_target = policy_target(torch.FloatTensor(state).to(device))\n",
    "        action = torch.multinomial(probs_target, 1).item()\n",
    "        \n",
    "        next_state, reward, terminated, _ = environment.step(action) \n",
    "        #must add:\n",
    "#         env.render()\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        if terminated:\n",
    "            break  \n",
    "        \n",
    "        state = next_state\n",
    "    return states, actions, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'hover'  # 'hover' or 'landing'\n",
    "version = 5\n",
    "\n",
    "max_m_episode = 200000\n",
    "max_steps = 800\n",
    "     #network and optimizer\n",
    "\n",
    "    #hyperparameters:\n",
    "alpha = 2.5e-4\n",
    "gamma = 0.99\n",
    "lmbda         = 0.99\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 4\n",
    "\n",
    "env = Rocket(task=task, max_steps=max_steps)\n",
    "\n",
    "\n",
    "    #create networks:\n",
    "pi = PolicyNetwork(env.state_dims, env.action_dims)\n",
    "pi_optimizer = torch.optim.Adam(pi.parameters(), lr=alpha)\n",
    "pi_target = PolicyNetwork(env.state_dims, env.action_dims)\n",
    "\n",
    "V = VNetwork(env.state_dims)\n",
    "V_optimizer = torch.optim.Adam(V.parameters(), lr=alpha)  \n",
    "\n",
    "V = V.to(device)\n",
    "pi = pi.to(device)\n",
    "pi_target = pi_target.to(device)\n",
    "\n",
    "    \n",
    "ckpt_folder = os.path.join('./', task + '_ckpt')\n",
    "if not os.path.exists(ckpt_folder):\n",
    "    os.mkdir(ckpt_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 0\n",
    "MAX_EPISODES = 20000\n",
    "reward_history =[]\n",
    "reward_history_100 = deque(maxlen=100)\n",
    "\n",
    "while episode < MAX_EPISODES:  # episode loop\n",
    "        \n",
    "    pi_target.load_state_dict(pi.state_dict())\n",
    "    states, actions, rewards = gen_episode(env, pi_target, device, max_steps)\n",
    "            \n",
    "    episode += 1    \n",
    "    for k in range(1,K_epoch):\n",
    "        loss1 = 0\n",
    "        loss2 = 0\n",
    "        GAE = 0\n",
    "        G = 0\n",
    "        for t in range(len(states) - 2, -1, -1):\n",
    "            S = states[t]\n",
    "            A = actions[t]\n",
    "            R = rewards[t]\n",
    "            S_next = states[t+1]\n",
    "                \n",
    "            S=torch.FloatTensor(S).to(device)\n",
    "            A=torch.tensor(A, dtype=torch.int8).to(device)\n",
    "            S_next=torch.FloatTensor(S_next).to(device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                delta = R + gamma*V(S_next)-V(S)\n",
    "                GAE = gamma * lmbda * GAE + delta           \n",
    "                G = gamma * G + R\n",
    "\n",
    "                # actor_output = pi(S)[A]\n",
    "                # actor_target_output = pi_target(S)[A]\n",
    "\n",
    "            actor_output = pi(S)\n",
    "            actor_target_output = pi_target(S)\n",
    "            actor_output = actor_output.view(-1).to(device)\n",
    "            actor_target_output = actor_target_output.view(-1).to(device)\n",
    "\n",
    "                \n",
    "                \n",
    "                # ratio = pi(S)[A]/pi_target(S)[A]\n",
    "            ratio = actor_output[A] / actor_target_output[A]\n",
    "                # print(\"ratio:\", ratio)\n",
    "            surr1 = ratio * (gamma**t)* GAE\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * (gamma**t)* GAE \n",
    "            loss1 = loss1 - torch.min(surr1, surr2)\n",
    "            loss2 = loss2 + (G - V(S))**2\n",
    "        loss2 = loss2/len(states)\n",
    "                \n",
    "        pi_optimizer.zero_grad()\n",
    "        loss1.backward()\n",
    "        pi_optimizer.step()\n",
    "            \n",
    "        V_optimizer.zero_grad()\n",
    "        loss2.backward()\n",
    "        V_optimizer.step() \n",
    "    \n",
    "    reward_history.append(G)\n",
    "        # reward_history_100.append(G)\n",
    "        # avg = sum(reward_history_100) / len(reward_history_100)\n",
    "\n",
    "    if episode % 10 == 1:\n",
    "        print('episode id: %d, episode return: %.3f'\n",
    "                % (episode, G))\n",
    "        plt.figure()\n",
    "        plt.plot(reward_history), plt.plot(utils.moving_avg(reward_history, N=50))\n",
    "        plt.legend(['episode reward', 'moving avg'], loc=2)\n",
    "        plt.xlabel('m episode')\n",
    "        plt.ylabel('return')\n",
    "        plt.savefig(os.path.join(ckpt_folder, 'rewards_' + str(version).zfill(8) + '.jpg'))\n",
    "        plt.close()\n",
    "\n",
    "        torch.save({'episode_id': episode,\n",
    "                            'REWARDS': reward_history,\n",
    "                            'model_pi_state_dict': pi.state_dict(),\n",
    "                            'model_V_state_dict': V.state_dict()},\n",
    "                           os.path.join(ckpt_folder, 'ckpt_' + str(version).zfill(8) + '.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history), plt.plot(utils.moving_avg(reward_history, N=50))\n",
    "plt.legend(['episode reward', 'moving avg'], loc=2)\n",
    "plt.xlabel('m episode')\n",
    "plt.ylabel('return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = 'hover'  # 'hover' or 'landing'\n",
    "# max_steps = 800\n",
    "# gamma = 0.99\n",
    "ckpt_dir = glob.glob(os.path.join(task+'_ckpt', '*.pt'))[-1]  # last ckpt\n",
    "\n",
    "print(ckpt_dir)\n",
    "env_test = Rocket(task=task, max_steps=max_steps)\n",
    "pi_test = PolicyNetwork(env.state_dims, env.action_dims)\n",
    "pi_test = pi_test.to(device)\n",
    "    \n",
    "if os.path.exists(ckpt_dir):\n",
    "    checkpoint = torch.load(ckpt_dir, map_location=torch.device(device))\n",
    "    pi.load_state_dict(checkpoint['model_pi_state_dict'])\n",
    "\n",
    "state = env_test.reset()\n",
    "episode_returns = list()\n",
    "for i in range(100):\n",
    "    _, _, rewards = gen_episode(env_test, pi_test, device, max_steps)\n",
    "    G = 0\n",
    "    for t in range(len(rewards) - 2, -1, -1):\n",
    "        R = rewards[t]\n",
    "        G = gamma * G + R\n",
    "\n",
    "    episode_returns.append(G)\n",
    "\n",
    "average_return = sum(episode_returns) / len(episode_returns)\n",
    "print(average_return / 17)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
