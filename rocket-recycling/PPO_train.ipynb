{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rocket import Rocket\n",
    "# from TestNetwork import ActorCritic\n",
    "from PPO_network import VNetwork, PolicyNetwork\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "from collections import deque\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode id: 1, episode return: 7.487\n",
      "episode id: 11, episode return: 10.174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/joonpark/Desktop/KAIST_Y4S2/강화학습개론/RL_FinalProject/rocket-recycling/PPO_train.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joonpark/Desktop/KAIST_Y4S2/%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%80%E1%85%A2%E1%84%85%E1%85%A9%E1%86%AB/RL_FinalProject/rocket-recycling/PPO_train.ipynb#W1sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m loss2 \u001b[39m=\u001b[39m loss2\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(states)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joonpark/Desktop/KAIST_Y4S2/%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%80%E1%85%A2%E1%84%85%E1%85%A9%E1%86%AB/RL_FinalProject/rocket-recycling/PPO_train.ipynb#W1sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m pi_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/joonpark/Desktop/KAIST_Y4S2/%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%80%E1%85%A2%E1%84%85%E1%85%A9%E1%86%AB/RL_FinalProject/rocket-recycling/PPO_train.ipynb#W1sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m loss1\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joonpark/Desktop/KAIST_Y4S2/%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%80%E1%85%A2%E1%84%85%E1%85%A9%E1%86%AB/RL_FinalProject/rocket-recycling/PPO_train.ipynb#W1sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m pi_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joonpark/Desktop/KAIST_Y4S2/%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%80%E1%85%A2%E1%84%85%E1%85%A9%E1%86%AB/RL_FinalProject/rocket-recycling/PPO_train.ipynb#W1sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m V_optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def gen_episode(environment, policy_target, device, max_steps = 800):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    ratios = []\n",
    "    state = environment.reset() \n",
    "    terminated = False\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        probs_target = policy_target(torch.FloatTensor(state).to(device))\n",
    "        action = torch.multinomial(probs_target, 1).item()\n",
    "        \n",
    "        next_state, reward, terminated, _ = environment.step(action) \n",
    "        #must add:\n",
    "#         env.render()\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        if terminated:\n",
    "            break  \n",
    "        \n",
    "        state = next_state\n",
    "    return states, actions, rewards\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    task = 'hover'  # 'hover' or 'landing'\n",
    "    version = 3\n",
    "\n",
    "    max_m_episode = 200000\n",
    "    max_steps = 800\n",
    "     #network and optimizer\n",
    "\n",
    "    #hyperparameters:\n",
    "    alpha = 2.5e-4\n",
    "    gamma = 0.99\n",
    "    lmbda         = 0.99\n",
    "    eps_clip      = 0.1\n",
    "    K_epoch       = 4\n",
    "\n",
    "    env = Rocket(task=task, max_steps=max_steps)\n",
    "\n",
    "\n",
    "    #create networks:\n",
    "    pi = PolicyNetwork(env.state_dims, env.action_dims)\n",
    "    pi_optimizer = torch.optim.Adam(pi.parameters(), lr=alpha)\n",
    "    pi_target = PolicyNetwork(env.state_dims, env.action_dims)\n",
    "\n",
    "    V = VNetwork(env.state_dims)\n",
    "    V_optimizer = torch.optim.Adam(V.parameters(), lr=alpha)  \n",
    "\n",
    "    V = V.to(device)\n",
    "    pi = pi.to(device)\n",
    "    pi_target = pi_target.to(device)\n",
    "\n",
    "    \n",
    "    ckpt_folder = os.path.join('./', task + '_ckpt')\n",
    "    if not os.path.exists(ckpt_folder):\n",
    "        os.mkdir(ckpt_folder)\n",
    "\n",
    "\n",
    "    # if len(glob.glob(os.path.join(ckpt_folder, '*.pt'))) > 0:\n",
    "    #         # load the last ckpt\n",
    "    #     checkpoint = torch.load(glob.glob(os.path.join(ckpt_folder, '*.pt'))[-1])\n",
    "    #     #modify later:\n",
    "    #     net.load_state_dict(checkpoint['model_G_state_dict'])\n",
    "    #     last_episode_id = checkpoint['episode_id']\n",
    "    #     REWARDS = checkpoint['REWARDS']\n",
    "\n",
    "\n",
    "    episode = 0\n",
    "    MAX_EPISODES = 20000\n",
    "    reward_history =[]\n",
    "    reward_history_100 = deque(maxlen=100)\n",
    "\n",
    "    while episode < MAX_EPISODES:  # episode loop\n",
    "        \n",
    "        pi_target.load_state_dict(pi.state_dict())\n",
    "        states, actions, rewards = gen_episode(env, pi_target, device)\n",
    "            \n",
    "        episode += 1    \n",
    "        for k in range(1,K_epoch):\n",
    "            loss1 = 0\n",
    "            loss2 = 0\n",
    "            GAE = 0\n",
    "            G = 0\n",
    "            for t in range(len(states) - 2, -1, -1):\n",
    "                S = states[t]\n",
    "                A = actions[t]\n",
    "                R = rewards[t]\n",
    "                S_next = states[t+1]\n",
    "                \n",
    "                S=torch.FloatTensor(S).to(device)\n",
    "                A=torch.tensor(A, dtype=torch.int8).to(device)\n",
    "                S_next=torch.FloatTensor(S_next).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    #general advantage estimation : GAE\n",
    "                    delta = R + gamma*V(S_next)-V(S)\n",
    "                    GAE = gamma * lmbda * GAE + delta           \n",
    "                    G = gamma * G + R\n",
    "\n",
    "                # actor_output = pi(S)[A]\n",
    "                # actor_target_output = pi_target(S)[A]\n",
    "\n",
    "                actor_output = pi(S)\n",
    "                actor_target_output = pi_target(S)\n",
    "                actor_output = actor_output.view(-1).to(device)\n",
    "                actor_target_output = actor_target_output.view(-1).to(device)\n",
    "\n",
    "                \n",
    "                \n",
    "                # ratio = pi(S)[A]/pi_target(S)[A]\n",
    "                ratio = actor_output[A] / actor_target_output[A]\n",
    "                # print(\"ratio:\", ratio)\n",
    "                surr1 = ratio * (gamma**t)* GAE\n",
    "                surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * (gamma**t)* GAE \n",
    "                loss1 = loss1 - torch.min(surr1, surr2)\n",
    "                loss2 = loss2 + (G - V(S))**2\n",
    "            loss2 = loss2/len(states)\n",
    "                \n",
    "            pi_optimizer.zero_grad()\n",
    "            loss1.backward()\n",
    "            pi_optimizer.step()\n",
    "            \n",
    "            V_optimizer.zero_grad()\n",
    "            loss2.backward()\n",
    "            V_optimizer.step() \n",
    "    \n",
    "        reward_history.append(G)\n",
    "        # reward_history_100.append(G)\n",
    "        # avg = sum(reward_history_100) / len(reward_history_100)\n",
    "\n",
    "        if episode % 10 == 1:\n",
    "            print('episode id: %d, episode return: %.3f'\n",
    "                % (episode, G))\n",
    "            plt.figure()\n",
    "            plt.plot(reward_history), plt.plot(utils.moving_avg(reward_history, N=50))\n",
    "            plt.legend(['episode reward', 'moving avg'], loc=2)\n",
    "            plt.xlabel('m episode')\n",
    "            plt.ylabel('reward')\n",
    "            plt.savefig(os.path.join(ckpt_folder, 'rewards_' + str(version).zfill(8) + '.jpg'))\n",
    "            plt.close()\n",
    "\n",
    "            torch.save({'episode_id': episode,\n",
    "                            'REWARDS': reward_history,\n",
    "                            'model_pi_state_dict': pi.state_dict(),\n",
    "                            'model_V_state_dict': V.state_dict()},\n",
    "                           os.path.join(ckpt_folder, 'ckpt_' + str(version).zfill(8) + '.pt'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
