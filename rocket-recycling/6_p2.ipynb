{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-09T11:25:47.655463Z","iopub.status.busy":"2023-06-09T11:25:47.654837Z","iopub.status.idle":"2023-06-09T11:25:48.936628Z","shell.execute_reply":"2023-06-09T11:25:48.935426Z"},"id":"POdyXtcNw4t_"},"outputs":[],"source":["import numpy as np\n","import torch\n","from rocket import Rocket\n","from torch.utils.data import DataLoader, TensorDataset, random_split # added\n","# from TestNetwork import ActorCritic\n","# from PPO_network import VNetwork, PolicyNetwork\n","import matplotlib.pyplot as plt\n","import utils\n","import os\n","import glob\n","import time\n","import datetime\n","from collections import deque\n","import torch.optim as optim\n","import random\n","import torch.nn as nn\n","\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # this line was added to avoid kernel error within my environment\n","\n","\n","# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-09T11:25:48.942609Z","iopub.status.busy":"2023-06-09T11:25:48.942079Z","iopub.status.idle":"2023-06-09T11:25:48.969865Z","shell.execute_reply":"2023-06-09T11:25:48.968737Z"},"id":"WcB_pmqYw4uD"},"outputs":[],"source":["\n","class PositionalMapping(nn.Module):\n","    \"\"\"\n","    Positional mapping Layer.\n","    This layer map continuous input coordinates into a higher dimensional space\n","    and enable the prediction to more easily approximate a higher frequency function.\n","    See NERF paper for more details (https://arxiv.org/pdf/2003.08934.pdf)\n","    \"\"\"\n","\n","    def __init__(self, input_dim, L=5, scale=1.0):\n","        super(PositionalMapping, self).__init__()\n","        self.L = L\n","        self.output_dim = input_dim * (L*2 + 1)\n","        self.scale = scale\n","\n","    def forward(self, x):\n","\n","        x = x * self.scale\n","\n","        if self.L == 0:\n","            return x\n","\n","        h = [x]\n","        PI = 3.1415927410125732\n","        for i in range(self.L):\n","            x_sin = torch.sin(2**i * PI * x)\n","            x_cos = torch.cos(2**i * PI * x)\n","            h.append(x_sin)\n","            h.append(x_cos)\n","\n","        return torch.cat(h, dim=-1) / self.scale\n","\n","# Based on the code from https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n","# Network architecture and hyperparameters are based on : https://arxiv.org/pdf/2006.05990.pdf\n","# The code below is taken from: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb\n","\n","\n","def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n","    nn.init.orthogonal_(layer.weight, std)\n","    nn.init.constant_(layer.bias, bias_const)\n","    return layer\n","\n","class NetworkLayer(nn.Module):\n","    def __init__(self, input_dim, output_dim) -> None:\n","        super().__init__()\n","        self.mapping = PositionalMapping(input_dim=input_dim, L=7)\n","        self.layer = nn.Sequential(\n","            layer_init(nn.Linear(self.mapping.output_dim, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, output_dim), std=1.0),\n","        )\n","    def forward(self, x):\n","        x = x.view([1, -1])\n","        x =  self.mapping(x)\n","        x = self.layer(x)\n","        return x\n","\n","class VNetwork(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.mapping = PositionalMapping(input_dim=input_dim, L=7)\n","        self.critic = nn.Sequential(\n","            layer_init(nn.Linear(self.mapping.output_dim, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 1), std=1.0),\n","        )\n","    def forward(self, x):\n","        #x = x.view([1, -1]) # removed\n","        x = self.mapping(x)\n","        x = self.critic(x)\n","        return x\n","\n","\n","class PolicyNetwork(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.mapping = PositionalMapping(input_dim=input_dim, L=7)\n","        self.actor = nn.Sequential(\n","            layer_init(nn.Linear(self.mapping.output_dim, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, output_dim), std=1.0),\n","        )\n","\n","    def forward(self, x):\n","        #x = x.view([1, -1]) # removed\n","        x = self.mapping(x)\n","        x = self.actor(x)\n","        x = nn.functional.softmax(x, dim=-1)\n","        return x\n","\n","def gen_episode(environment, policy_target, device, max_step = 800):\n","    states = []\n","    actions = []\n","    rewards = []\n","\n","    state = environment.reset()\n","    terminated = False\n","    eps = 0.01\n","    for step in range(max_step):\n","        probs_target = policy_target(torch.FloatTensor(state).to(device))\n","        if random.random() < eps:  # exploration\n","            action = random.randint(0, environment.action_dims - 1)\n","        else:\n","            action = torch.multinomial(probs_target, 1).item()\n","        next_state, reward, terminated, _ = environment.step(action)\n","        states.append(state)\n","        actions.append(action)\n","        rewards.append(reward)\n","        if terminated:\n","            break\n","        state = next_state\n","    return states, actions, rewards"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-09T11:25:48.974748Z","iopub.status.busy":"2023-06-09T11:25:48.974368Z","iopub.status.idle":"2023-06-09T11:25:49.891129Z","shell.execute_reply":"2023-06-09T11:25:49.890009Z"},"id":"6jVUvyY2w4uE"},"outputs":[],"source":["task = 'landing'  # 'hover' or 'landing'\n","version = 5\n","\n","max_steps = 800\n","\n","#hyperparameters:\n","alpha = 2.5e-4\n","gamma = 0.99\n","lmbda         = 0.99\n","eps_clip      = 0.2\n","K_epoch       = 4\n","entropy_coeff = 0.01\n","SAVE_INTERVAL = 1000\n","mini_batch_size = 64\n","env = Rocket(task=task, max_steps=max_steps)\n","\n","\n","\n","#create networks:\n","pi = PolicyNetwork(env.state_dims, env.action_dims)\n","pi_optimizer = torch.optim.Adam(pi.parameters(), lr=alpha)\n","pi_target = PolicyNetwork(env.state_dims, env.acenv_cls = \"ICCGANHumanoidTarget\"\n","env_params = dict(\n","    episode_length = 500,\n","    motion_file = \"assets/motions/clips_walk.yaml\",\n","    goal_reward_weight = [0.5],\n","\n","    goal_radius = 0.5,\n","    sp_lower_bound = 1.2,\n","    sp_upper_bound = 1.5,\n","    goal_timer_range = (90, 150),\n","    goal_sp_mean = 1,\n","    goal_sp_std = 0.25,\n","    goal_sp_min = 0,\n","    goal_sp_max = 1.25\n",")\n","\n","training_params = dict(\n","    max_epochs = 100000,\n","    save_interval = 10000,\n","    terminate_reward = -25\n",")\n","\n","discriminators = {\n","    \"walk/full\": dict(\n","        parent_link = None,\n","    )\n","}\n","tion_dims)\n","\n","V = VNetwork(env.state_dims)\n","V_optimizer = torch.optim.Adam(V.parameters(), lr=alpha)\n","\n","parameters = list(pi.parameters()) + list(V.parameters())\n","# total_params = nn.ParameterList(parameters=parameters)\n","total_params = nn.ParameterList(parameters)\n","\n","optimizer = torch.optim.Adam(total_params, lr=alpha)\n","\n","\n","V = V.to(device)\n","pi = pi.to(device)\n","pi_target = pi_target.to(device)\n","\n","\n","ckpt_folder = os.path.join('./', task + '_ckpt')\n","if not os.path.exists(ckpt_folder):\n","    os.mkdir(ckpt_folder)"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-09T11:25:49.895848Z","iopub.status.busy":"2023-06-09T11:25:49.895551Z","iopub.status.idle":"2023-06-10T02:22:48.200680Z","shell.execute_reply":"2023-06-10T02:22:48.200041Z"},"id":"4GhfLQwmw4uF","outputId":"d99c8d3c-c8bb-4795-951a-a0b68835c77c"},"outputs":[{"ename":"NameError","evalue":"name 'deque' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m MAX_EPISODES \u001b[39m=\u001b[39m \u001b[39m20000\u001b[39m\n\u001b[1;32m      3\u001b[0m reward_history \u001b[39m=\u001b[39m[]\n\u001b[0;32m----> 4\u001b[0m reward_history_100 \u001b[39m=\u001b[39m deque(maxlen\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mwhile\u001b[39;00m episode \u001b[39m<\u001b[39m MAX_EPISODES:  \u001b[39m# episode loop\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     pi_target\u001b[39m.\u001b[39mload_state_dict(pi\u001b[39m.\u001b[39mstate_dict())\n","\u001b[0;31mNameError\u001b[0m: name 'deque' is not defined"]}],"source":["episode = 0\n","MAX_EPISODES = 20000\n","reward_history =[]\n","reward_history_100 = deque(maxlen=100)\n","\n","while episode < MAX_EPISODES:  # episode loop\n","\n","    pi_target.load_state_dict(pi.state_dict())\n","    states, actions, rewards = gen_episode(env, pi_target, device, max_steps)\n","\n","    GAEs = []\n","    Gs = []\n","    GAE = 0\n","    G = 0\n","\n","    values = V(torch.FloatTensor(np.array(states)).to(device)).view(-1)\n","    for t in reversed(range(len(rewards))):\n","        if t < len(rewards) - 1:\n","            delta = rewards[t] + gamma * values[t + 1] - values[t]\n","        else:\n","            delta = rewards[t] - values[t]\n","        GAE = gamma * lmbda * GAE + delta\n","        GAEs.insert(0, GAE)\n","        G = gamma * G + rewards[t]\n","        Gs.insert(0, G)\n","\n","    GAEs = torch.FloatTensor(GAEs).to(device)\n","    Gs = torch.FloatTensor(Gs).to(device)\n","\n","    states = np.array(states)\n","    states = torch.FloatTensor(np.array(states)).to(device)\n","    actions = torch.LongTensor(actions).to(device)\n","\n","    dataset = TensorDataset(states, actions, GAEs, Gs)\n","    dataloader = DataLoader(dataset, batch_size=mini_batch_size, shuffle=True)\n","\n","    episode += 1\n","    for k in range(1,K_epoch):\n","        for batch_states, batch_actions, batch_GAEs, batch_Gs in dataloader:\n","            # policy loss\n","            loss1 = 0\n","            # value loss\n","            loss2 = 0\n","            entropy_mean = 0\n","            values = V(batch_states).view(-1)\n","\n","            actor_outputs = pi(batch_states)\n","            actor_target_outputs = pi_target(batch_states)\n","\n","            for t in range(len(batch_states)):\n","                A = batch_actions[t]\n","                ratio = actor_outputs[t][A] / actor_target_outputs[t][A]\n","\n","                entropy_mean -= torch.distributions.Categorical(probs=actor_outputs[t]).entropy()\n","\n","                surr1 = ratio * batch_GAEs[t]\n","                surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * batch_GAEs[t]\n","                loss1 = loss1 - torch.min(surr1, surr2)\n","\n","            # policy loss\n","            entropy_loss = entropy_mean / len(batch_states)\n","            loss1 = loss1 + entropy_coeff * entropy_loss\n","\n","            # value loss\n","            loss2 = ((batch_Gs - values) ** 2).mean()\n","\n","            total_loss = loss1 + 0.5 * loss2\n","            optimizer.zero_grad()\n","            total_loss.backward()\n","            optimizer.step()\n","\n","    reward_history.append(G)\n","\n","    if episode % 20 == 1:\n","        print('episode id: %d, episode return: %.3f'\n","                % (episode, G))\n","        if episode % 1000 == 1:\n","            plt.figure()\n","            plt.plot(reward_history), plt.plot(utils.moving_avg(reward_history, N=50))\n","            plt.legend(['episode reward', 'moving avg'], loc=2)\n","            plt.xlabel('m episode')\n","            plt.ylabel('return')\n","            plt.savefig(os.path.join(ckpt_folder, 'rewards_' + str(version).zfill(8) + '.jpg'))\n","            plt.close()\n","\n","    if episode % 50 == 1:\n","        torch.save({'episode_id': episode,\n","                            'REWARDS': reward_history,\n","                            'model_pi_state_dict': pi.state_dict(),\n","                            'model_V_state_dict': V.state_dict(),\n","                            'optimizer': optimizer.state_dict()},\n","                            os.path.join(ckpt_folder, 'ckpt_' + str(version).zfill(8) + '.pt'))\n","\n","    if episode % SAVE_INTERVAL == 0:\n","        torch.save({'episode_id': episode,\n","                            'REWARDS': reward_history,\n","                            'model_pi_state_dict': pi.state_dict(),\n","                            'model_V_state_dict': V.state_dict(),\n","                            'optimizer': optimizer.state_dict()},\n","                            os.path.join(ckpt_folder, 'ckpt_' + str(episode).zfill(8) + '.pt'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T02:22:48.206481Z","iopub.status.busy":"2023-06-10T02:22:48.205846Z","iopub.status.idle":"2023-06-10T02:22:48.376815Z","shell.execute_reply":"2023-06-10T02:22:48.376244Z"},"id":"Ys3dZxVdw4uG","outputId":"c6e35188-4490-4d78-c0bd-8e6fdb39a466"},"outputs":[],"source":["plt.plot(reward_history), plt.plot(utils.moving_avg(reward_history, N=50))\n","plt.legend(['episode reward', 'moving avg'], loc=2)\n","plt.xlabel('m episode')\n","plt.ylabel('return')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T02:22:48.381496Z","iopub.status.busy":"2023-06-10T02:22:48.381212Z","iopub.status.idle":"2023-06-10T02:23:06.581205Z","shell.execute_reply":"2023-06-10T02:23:06.580617Z"},"id":"nWjtdOeDw4uH","outputId":"408e744d-9360-475f-dc62-711fa5ceda51"},"outputs":[],"source":["task = 'hover'  # 'hover' or 'landing'\n","max_steps = 800\n","gamma = 0.99\n","ckpt_dir = 'hover_ckpt/ckpt_00020000.pt'\n","\n","print(ckpt_dir)\n","env_test = Rocket(task=task, max_steps=max_steps)\n","pi_test = PolicyNetwork(env_test.state_dims, env_test.action_dims)\n","pi_test = pi_test.to(device)\n","\n","if os.path.exists(ckpt_dir):\n","    checkpoint = torch.load(ckpt_dir, map_location=torch.device(device))\n","    pi_test.load_state_dict(checkpoint['model_pi_state_dict'])\n","\n","state = env_test.reset()\n","episode_returns = list()\n","for i in range(100):\n","    _, _, rewards = gen_episode(env_test, pi_test, device, max_steps)\n","    G = 0\n","    for t in range(len(rewards) - 1, -1, -1):\n","        R = rewards[t]\n","        G = gamma * G + R\n","\n","    episode_returns.append(G)\n","\n","average_return = sum(episode_returns) / len(episode_returns)\n","print(average_return / 120)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"deeprl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":0}
